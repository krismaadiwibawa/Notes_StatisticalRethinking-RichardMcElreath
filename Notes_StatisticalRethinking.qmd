---
title: "Notes_StatisticalRethinking"
author: "Krisma Adiwibawa"
format: gfm
editor:
  markdown: 
    wrap: sentence
editor_options:
  chunk_output_type: console
---

```{r}
library(rethinking)
```

## Chapter 2: Small Worlds and Large Worlds

### Grid Approximation in the Globe Tossing Context

```{r}
# Step 1: Define grid
p_grid <- seq(0, 1, length.out=20)
# grid of 20 points. They act as the candidates of the parameter value. We know the proportion of water is between 0 (0 %, no water) and 1 (100%, all water). It's impossible to have a negative value or a value of more than 1, so we limit the candidates values to this range.
# We even know already that there is more water than land actually. But here we pretend we are dumb and we don't have this idea.

# Step 2: Define prior
prior <- rep(1, length(p_grid))
#### Prior alternatives:
# prior <- ifelse(p_grid < 0.5, 0, 1)
# We assign the prior likelihood of 0 to candidates < 0.5 and the prior likelihood of 1 to candidates >= 0.5
# or
# prior <- exp(-5 * abs(p_grid-0.5))
# exponential decay function / Laplace distribution. Pointy in the middle and has heavier tails than a Gaussian distribution. This is a prior which puts a higher likelihood to the middle value of range 0 and 1

# Step 3: Compute likelihood at each value in grid
likelihood <- dbinom(6, 9, p_grid)
# This is where the actual data informs us about the likelihood of each candidate (each element of p_grid). The data would suggest that some elements of p_grid are more likely than others.

# Step 4: Compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# This is where our prior belief is updated by incorporating information we get from the data.
# prior <- rep(1, length(p_grid)) means "1" is multiplied to each element in the vector 'likelihood'. In other words, our prior is giving equal likelihood to each candidate because we didn't have any idea which candidate is more likely. Now, we are taking the likelihoods  in the vector 'likelihood' as our 'posterior' (updated belief).

# Step 5: Standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# Result:
# Now we have the updated likelihood after seeing the data, namely 'posterior'.
# Explanation:
# First, we nominate 20 points between 0 and 1 as the candidates. We have no idea which is the most likely to be true parameter value.
# After seeing the data, we get some information about the likelihood of the candidates.
# We then update our prior with this information by multiplying the prior with the information we got from the data. In this case, all parameter candidate (the 20 points in the grid is equally likely (all have a prior of 1).
# Now, some candidates are more likely than others because the data informs us so. These likelihoods is contained in the vector 'posterior'.
```

### Visualization of the likelihood (posterior probability) of each 20 candidate

```{r}
plot(p_grid, posterior, type="b", xlab = "probability of water", ylab = "posterior probability", main="20 points")
```

### Quadratic Approximation of the Globe Tossing Context (Try Out)

```{r}
library(rethinking)

globe.qa <- quap(
  alist(
    W ~ dbinom(W+L, p), # binomial likelihood
    p ~ dunif(0,1) # uniform prior
    ),
    data=list(W=6, L=3)
)

# display summary of quadratic approximation
precis(globe.qa)
```

### Compare How Good the Quadratic Approximation is, With the Grid Approximation Result

```{r}
# analytical calculation
W <- 6
L <- 3
curve(dbeta(x, W+1, L+1), from=0, to=1)

# quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty=2, add=TRUE)
```

### Markov Chain Monte Carlo in the Globe Tossing Context (Try Out)

```{r}
n_samples <- 1000
p <- rep(NA, n_samples)
p[1] <- 0.5
W <- 6
L <- 3
for (i in 2:n_samples) {
    p_new <- rnorm(1, p[i-1], 0.1)
    if (p_new < 0) p_new <- abs(p_new)
    if (p_new > 1) p_new <- 2- p_new
    q0 <- dbinom(W, W+L, p[i-1])
    q1 <- dbinom(W, W+L, p_new)
    p[i] <- ifelse(runif(1) < q1/q0, p_new, p[i-1])
}
```

#### The values in p are samples from the posterior distribution. To compare to the analytical posterior:

```{r}
dens(p, xlim=c(0,1))
curve(dbeta(x, W+1, L+1), lty=2, add=TRUE)
```

## Chapter 3: Sampling the Imaginary

### Vampire test

```{r}
Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001
Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1-Pr_Vampire)

(Pr_Vampire_Positive <- (Pr_Positive_Vampire * Pr_Vampire) / Pr_Positive)
# 8.7% chance that the suspect is actually a vampire


```

### Sampling from a grid-approximate posterior

```{r}
p_grid <- seq(from=0, to=1, length.out=10000)
prob_p <- rep(1, 10000)
prob_data <- dbinom(6, size=9, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)
```

### Now, we want to take 10000 samples from the posterior

```{r}
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
View(table(samples))
plot(samples)

dens(samples)
```

### Ch. 3 - Practice

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size=9, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)

# How much posterior probability lies below p = 0.2?
(sum(samples < 0.2)/1e4)

# How much posterior probability lies above p = 0.8?
(sum(samples > 0.8)/1e4)

# How much posterior probability lies between p = 0.2 and p = 0.8?
(sum(samples > 0.2 & samples < 0.8)/1e4)

# 20% of the posterior probability lies below which value of p?
(quantile(samples, 0.2))

# 20% of the posterior probability lies above which value of p?
(quantile(samples, 0.8))

# Which values of p contain the narrowest interval equal to 66% of the posterior probability?
(HPDI(samples, prob=0.66))

# Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)
likelihood <- dbinom(8, size=15, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior/sum(posterior)

plot(p_grid, posterior, type="b",
     xlab="probability of water", ylab="posterior probability")
mtext("20 points")

# Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.

samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)

(HPDI(samples, prob=0.9))

plot(samples)
dens(samples)
precis(samples)

# Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?

post_pred_check <- rbinom(1e4, size=15, prob=samples)
(mean(post_pred_check == 8))
simplehist(post_pred_check, xlab="posterior predictive distribution")

# Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses

prob_6_in_9 <- sum( dbinom(6, size=9, prob=p_grid) * posterior )
prob_6_in_9

# or

prob_6_in_9 <- mean( dbinom(6, size=9, prob=samples) )
prob_6_in_9

# visualization
set.seed(123)
post_pred_samples <- rbinom(10000, size=9, prob=samples)
simplehist(post_pred_samples, xlab="Number of water tosses")

## R code 3.28
birth1 <- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,
0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,
1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,
1,0,1,1,1,0,1,1,1,1)
birth2 <- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,
1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,
1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,
0,0,0,1,1,1,0,0,0,0)
# These data indicate the gender (male=1, female=0) of officially reported first and second born children in 100 two-child families.

(sum(birth1)+sum(birth2))

# Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform probability. Which parameter value maximizes the posterior probability?

p_grid <- seq(0, 1, length.out=1000)
prior <- rep(1, length(p_grid))
# prior <- rep(1, length(p_grid)) This corresponds to a uniform distribution or beta(1,1), which

n_boys <- sum(birth1)+sum(birth2)
n_total <- length(birth1)+length(birth2)

likelihood <- dbinom(n_boys, n_total, prob= p_grid)

posterior <- likelihood * prior
posterior <- posterior/sum(posterior)

# Plot posterior
plot(p_grid, posterior, type = "l", lwd = 2, col = "blue",
     xlab = "Probability of a boy (p)",
     ylab = "Posterior probability",
     main = "Posterior Distribution of p (Grid Approximation)")

# Which parameter value maximizes the posterior probability?
p_MAP <- p_grid[which.max(posterior)]
p_MAP
abline(v = p_MAP, col = "red", lty = 2)
text(p_MAP, max(posterior), labels = paste0("MAP=", round(p_MAP, 3)),
     pos = 4, col = "red")

# Using the 'sample' function, draw 10000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.

set.seed(123)
posterior_samples <- sample(p_grid, size=10000, replace=TRUE, prob=posterior)

HPDI(posterior_samples, prob=0.5)
HPDI(posterior_samples, prob=0.89)
HPDI(posterior_samples, prob=0.97)

# Use 'rbinom' to simulate 10000 replicates of 200 births. You should end up with 10000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births. There are many good ways to visualize the simulations, but the 'dens' command (part of 'rethinking' package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?

set.seed(123)
posterior_predictive_dist_boys <- rbinom(n=10000, size=200, prob=posterior_samples)
# this is creating a 10000 'counts of boys out of 200 births'. Each element in the posterior_samples vector (10000 parameter candidates, whose distribution reflects the updated probabilities of each parameter candidate) determines the count of boys for each element of 'posterior_predictive_dist_boys'.

simplehist(posterior_predictive_dist_boys, main="Predicted Distribution of Boys (out of 200 Births)", xlab="Number of Boys")

abline(v = 111, col = "red", lwd = 2)
text(111, max(table(posterior_predictive_dist_boys))*0.97, "Actual: 111", col="red", pos=4)


# Now compare 10000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?

actual_first_boys <- sum(birth1)

set.seed(123)
first_boys_simulation <- rbinom(n=10000, size=100, prob=posterior_samples)

simplehist(first_boys_simulation, xlab="Number of boys (first borns", ylab="Frequency", main="Posterior Predictive Distribution for First-Borns")

abline(v = actual_first_boys, col = "red", lwd = 2)
text(actual_first_boys, max(table(first_boys_simulation))*0.9, 
     paste("Actual:", actual_first_boys), col="red", pos=2)

# The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?

girls_first <- which(birth1==0) # index of girl births
num_girls_first <- length(girls_first) # total number of girl births

actual_boys_after_girls <- sum(birth2[girls_first]) # total number of boys born after a girl

set.seed(123)
sim_boys_after_girls <- rbinom(n=10000, size=num_girls_first, prob=posterior_samples)

simplehist(sim_boys_after_girls, 
           xlab = "Number of boys (second-borns after girls)", 
           ylab = "Frequency", 
           main = "Posterior Predictive for Second-Borns after Girls")

abline(v = actual_boys_after_girls, col = "red", lwd = 2)
text(actual_boys_after_girls, max(table(sim_boys_after_girls))*0.9, 
     paste("Actual:", actual_boys_after_girls), col="red", pos=2)


# Trying out creating a model predicting the birth of boys after girls
index_girls_first <- which(birth1==0)
num_girls_first <- length(index_girls_first)
actual_boys_after_girls <- sum(birth2[index_girls_first])

set.seed(123)
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, length(p_grid))

likelihood <- dbinom(actual_boys_after_girls, num_girls_first, prob=p_grid)
posterior <- likelihood * prior
posterior <- likelihood/sum(likelihood)

posterior_samples <- sample(p_grid, prob=posterior, size=10000, replace=TRUE)

set.seed(123)
posterior_predictive_simulation <- rbinom(10000, size=num_girls_first, prob=posterior_samples)

simplehist(posterior_predictive_simulation,
           xlab="Number of boys (second-born",
           ylab="Frequency",
           mein="Posterior Predictive for Second-Born Boys after Girls")
abline(v = actual_boys_after_girls, col='red', lwd=2)
text(actual_boys_after_girls, max(table(posterior_samples)),
     paste("Actual:", actual_boys_after_girls), col='red', pos=2)

# So, if the data to be predicted has some assumption that is incorporated in the model, the model is going to predict the data well.
```

## Chapter 4 - Geocentric Models

### Grid Approximation applied to a model with more than one parameter

```{r}
pos <- replicate(1000, sum(runif(16,-1,1)))

hist(pos)
plot(density(pos))
```

```{r}
growth <- replicate(10000, prod(1+runif(12, 0, 0.1)))
dens(growth, norm.comp=TRUE)
```

```{r}
big <- replicate(10000, prod(1+(runif(12, 0, 0.5))))
small <- replicate(10000, prod(1+(runif(12, 0, 0.1))))

dens(big, norm.comp=TRUE)
dens(small, norm.comp=TRUE)
```

```{r}
log.big <- replicate(10000, log(prod(1+(runif(12, 0, 0.5)))))
dens(log.big, norm.comp=TRUE)
```

```{r}
w <- 6; n <- 9
p_grid <- seq(from=0, to=1, length.out=100)
posterior <- dbinom(w, n, p_grid) * dunif(p_grid, 0, 1)
posterior <- posterior/sum(posterior)
# dunif(p_grid, 0, 1) = d --> density, it is giving the probability density function (PDF), in this case, of a uniform distribution. So for every p_grid it gives probability of '1'.

samples <- sample(p_grid, size=10000, replace=TRUE, prob=posterior)

# plot the p_grid and posterior probability
plot(p_grid, posterior, type='b', xlab="p_grid", ylab="posterior probability")

# plot the simulated samples of 10000 p_grid with probabilities from the posterior
dens(samples)

# summary metrics
p_grid[which.max(posterior)]
# MAP (maximum a posteriori) from posterior / the index of p_grid which has the highest posterior
chainmode(samples, adj=0.01)
# MAP (maximum a posteriori) from samples / find the mode of a continuous density estimate
HPDI(samples, 0.05) # Highest Posterior Density Interval
PI(samples, 0.05) # Percentiles Interval
```

```{r}
library(rethinking)
data(Howell1)
# str(Howell1)
d <- Howell1
# precis(d)

d$height # return the column 'height' only from data frame 'd'

d2 <- d[d$age >= 18,] # filter only the rows where the age is >= 18

# plot the distribution of heights
dens(d2$height)

# plot the prior of height
curve(dnorm(x,, 178, 20), from=100, to=250)

# plot the prior of standard deviation (flat and uniform, just to constrain SD to have a positive probability between zero and 50cm)
curve(dunif(x, 0, 50), from=10, to=60)

# simulating heights by sampling from the prior
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)

# Grid Approximation
mu.list <- seq(from=150, to=160, length.out=100)
sigma.list <- seq(from=7, to=9, length.out=100)
post <- expand.grid(mu=mu.list, sigma=sigma.list)
post$LL <- sapply(1:nrow(post),
                  function(i) sum(
                        dnorm(d2$height, post$mu[i], post$sigma[i], log=TRUE)
                  ))
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
# posterior resides in post$prob

# simple contour plot
contour_xyz(post$mu, post$sigma, post$prob)

# simple heat map
image_xyz(post$mu, post$sigma, post$prob)

# Again, we sample the parameter values of p from the posterior. But because we have 2 parameters here we want to sample combinations of them, the new trick is:
# first, randomly sample row numbers in 'post' in proportion to the values in 'post$prob'.
# Then, we pull out the parameter values on those randomly sampled rows.

# This code will do it:
sample.rows <- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
# You end up with 10000 samples, with replacement, from the posterior for the height data.

plot(sample.mu, sample.sigma, cex=1, pch=16, col=col.alpha(rangi2, 0.22))
# Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of mu and sigma.

# Now that you have these samples, you can describe the distribution of confidence in each  combination of mu and sigma by summarizing the samples.

# To characterize the shapes of the 'marginal' posterior densities of mu and sigma, all we need to do is:
dens(sample.mu)
dens(sample.sigma)

# to summarize the widths of these densities with posterior compatibility intervals:
PI(sample.mu)
PI(sample.sigma)
# Since these samples are just vectors of numbers, you can compute any statistic from them that you could from ordinary data: mean, median, or quantile, for example.
summary(sample.mu)
summary(sample.sigma)
```

### Quadratic Approximation to the same model (parameters: mu and sigma)

```{r}
# Quadratic Approximation's goal is to quickly make inferences about the shape of the posterior. The posterior's peak will lie at the Maximum A Posteriori (MAP) estimate, and we can get a useful image of the posterior's shape by using the quadratic approximation of the posterior distribution at this peak.

library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]

# Now we define the model using R's formula syntax:
flist <- alist(
           height ~ dnorm(mu, sigma),
           mu ~ dnorm(178, 20),
           sigma ~ dunif(0, 50)
)

# Now we fit the model to the data in the data frame d2 with:
m4.1 <- quap(flist, data=d2)

# AFter executing this code, you'll have a fit model stored in the symbol m4.1. Now take a look at the posterior:
precis(m4.1)
# These numbers provide Gaussian approximations for each parameter's 'marginal' distribution. This means the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4

# Sampling from quap.
# How do you then get samples from the quadratic approximation of the posterior distribution? The answer is rather simple, but non-obvious, and it requires recognizing that a quadratic approximation to a posterior distribution with more  than one parameter dimension – mu and sigma each contribute one dimension – is just a multi-dimensional Gaussian distribution.
# As a consequence, when R constructs a quadratic approximation, it calculates not only standard deviations for all parameters, but also the covariances among all pairs of  parameters.
# To see this matrix of variances and covariances, for model m4.1, use:
vcov(m4.1)

# It is composed of
# (1) a vector of variances for the parameters
diag(vcov(m4.1))
# (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in others
cov2cor(vcov(m4.1))

# So how do we get samples from this multi-dimensional posterior? Now instead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution. The 'rethinking' package provides a convenience function to do just that:
library(rethinking)
post <- extract.samples(m4.1, n=1e4)
head(post)
# You end up with a data frame, post, with 10000 (1e4) rows and two columns, one column for mu and one for sigma. Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the MAP values from before. You can confirm this by summarizing the samples:
precis(post)

# We can plot it too:
plot(post)

# The high degree of similarity between the 'post' from quadratic approximation and 'm4.1 from grid approximation is because these samples also preserve the covariance between mu and sigma.
```

When a predictor variable is built inside the model in a particular way , we'll have a linear regression.
So, now let's look at how height in these Kalahari foragers (the outcome variable) covaries with weight (the predictor variable).

Go ahead and plot adult height and weight against one another:

```{r}
library(rethinking)
data(Howell1)

d <- Howell1
d2 <- d[d$age >=18,]

plot (d2$height ~ d2$weight)
```

How do we take our Gaussian model from the previous section and incorporate predictor variables?

The linear model strategy.
The strategy is to make the parameter for the mean of a Gaussian distribution, mu, into a linear function of the predictor variable and other, new parameters that we invent.

We ask the golem: "Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data." The golem answers with a posterior distribution.

```{r}
# define the average weight, x-bar
xbar <- mean(d2$weight)

# fit model
m4.3 <- quap(
        alist(
          height ~ dnorm(mu, sigma),
          mu <- a + b*(weight-xbar),
          a ~ dnorm(178, 20),
          b ~ dlnorm(0,1), # dlnorm is log-normal distribution to enforce positive values; see p. 96.
          sigma ~ dunif(0,50)),
        data=d2)


```
